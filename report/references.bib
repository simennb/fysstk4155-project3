@misc{Github1,
	title = "Git{H}ub repository, {P}roject 1",
	howpublished = "\\\url{https://github.com/simennb/fysstk4155-project1}"
}

@misc{Github2,
	title = "Git{H}ub repository, {P}roject 2",
	howpublished = "\\\url{https://github.com/simennb/fysstk4155-project2}"
}

@misc{Github3,
	title = "Git{H}ub repository, {P}roject 3",
	howpublished = "\\\url{https://github.com/simennb/fysstk4155-project3}"
}

@misc{MLPClassifier,
	title = "Scikit-{L}earn: {MLPC}lassifier",
	howpublished = "\\\url{https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html}",
	note = {(accessed 10. December 2020)}
}

@misc{Kaggle_catdog,
	title = "{A}udio {C}ats and {D}ogs",
	author = "{M}arc {M}oreaux",
	howpublished = "\\\url{https://www.kaggle.com/mmoreaux/audio-cats-and-dogs}",
	note = {(accessed 20. November 2020)}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@inproceedings{Takahashi+2016,
author={Naoya Takahashi and Michael Gygli and Beat Pfister and Luc Van Gool},
title={Deep Convolutional Neural Networks and Data Augmentation for Acoustic Event Recognition},
year=2016,
booktitle={Interspeech 2016},
doi={10.21437/Interspeech.2016-805},
url={http://dx.doi.org/10.21437/Interspeech.2016-805},
pages={2982--2986}
}

@article{Chen_2016,
   title={XGBoost},
   ISBN={9781450342322},
   url={http://dx.doi.org/10.1145/2939672.2939785},
   DOI={10.1145/2939672.2939785},
   journal={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   publisher={ACM},
   author={Chen, Tianqi and Guestrin, Carlos},
   year={2016},
   month={Aug}
}


@misc{Nielsen,
	title = "{Michael A. Nielsen}. \textit{{N}eural {N}etworks and {D}eep {L}earning} - {C}hapter 2",
	howpublished = "\\\url{http://neuralnetworksanddeeplearning.com/chap2.html}"
}

@misc{c2012,
      title={\textit{Multi-column {Deep} {Neural} {Networks} for {Image Classification}}}, 
      author={Dan Ciresan and Ueli Meier and Juergen Schmidhuber},
      year={2012},
      eprint={1202.2745},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@book{Hastie,
	author = {Hastie et al.},
	title = {The Elements of Statistical Learning - Data Mining, Inference, and 		Prediction},
	publisher = {Springer},
	year = {2017},
	edition = {Second edition},
}

@book{Geron,
	author = {Aurélien Géron},
	title = {Hands-On Machine Learning with Scikit-Learn and TensorFlow},
	publisher = {O'Reilly},
	year = {2017},
	edition = {First edition},
}



@InProceedings{glorot, title = {Understanding the difficulty of training deep feedforward neural networks}, author = {Xavier Glorot and Yoshua Bengio}, pages = {249--256}, year = {2010}, editor = {Yee Whye Teh and Mike Titterington}, volume = {9}, series = {Proceedings of Machine Learning Research}, address = {Chia Laguna Resort, Sardinia, Italy}, month = {13--15 May}, publisher = {JMLR Workshop and Conference Proceedings}, pdf = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf}, url = {http://proceedings.mlr.press/v9/glorot10a.html}, abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.} }

@article{roc_auc,
    author = {Hanczar, Blaise and Hua, Jianping and Sima, Chao and Weinstein, John and Bittner, Michael and Dougherty, Edward R.},
    title = "{Small-sample precision of ROC-related estimates}",
    journal = {Bioinformatics},
    volume = {26},
    number = {6},
    pages = {822-830},
    year = {2010},
    month = {02},
    abstract = "{Motivation: The receiver operator characteristic (ROC) curves are commonly used in biomedical applications to judge the performance of a discriminant across varying decision thresholds. The estimated ROC curve depends on the true positive rate (TPR) and false positive rate (FPR), with the key metric being the area under the curve (AUC). With small samples these rates need to be estimated from the training data, so a natural question arises: How well do the estimates of the AUC, TPR and FPR compare with the true metrics?Results: Through a simulation study using data models and analysis of real microarray data, we show that (i) for small samples the root mean square differences of the estimated and true metrics are considerable; (ii) even for large samples, there is only weak correlation between the true and estimated metrics; and (iii) generally, there is weak regression of the true metric on the estimated metric. For classification rules, we consider linear discriminant analysis, linear support vector machine (SVM) and radial basis function SVM. For error estimation, we consider resubstitution, three kinds of cross-validation and bootstrap. Using resampling, we show the unreliability of some published ROC results.Availability: Companion web site at http://compbio.tgen.org/paper\_supp/ROC/roc.htmlContact:edward@mail.ece.tamu.edu}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btq037},
    url = {https://doi.org/10.1093/bioinformatics/btq037},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/26/6/822/676553/btq037.pdf},
}

@misc{phan2017audio,
      title={Audio Scene Classification with Deep Recurrent Neural Networks}, 
      author={Huy Phan and Philipp Koch and Fabrice Katzberg and Marco Maass and Radoslaw Mazur and Alfred Mertins},
      year={2017},
      eprint={1703.04770},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}

